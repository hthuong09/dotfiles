#!/bin/env python

import sys
import os
import requests
from bs4 import BeautifulSoup

if len(sys.argv) < 2:
    print('You must enter an url')
    sys.exit(0)

url = sys.argv[1];

#cookie = {"LOGGED_USER": "m%2BVwpLzhxuFB8gCivKc15A%3D%3D%3AiVT59k7WMO1EytQSVWzyUg%3D%3D"}
html = requests.get(url).text

parser = BeautifulSoup(html, 'html.parser')

title = parser.find("h1", class_="js-post-title")

images = parser.find_all("img", class_="detail_std")

if len(images) == 0:
    print('Could not find any images')
    sys.exit(0)

profile = parser.find("a", class_="maxw180")


if profile == None:
    print('Could not find user profile')
    sys.exit(0)

cosplayer_name = profile.text
cosplayer_id = profile['href'].split('/')[-1]

directory = title.text.strip()

if not os.path.isdir(directory):
    os.mkdir(directory)
os.chdir(directory)

# create gallery information file
f = open(cosplayer_name + ' - bcy' + cosplayer_id, 'a')
f.write(url)
f.close()



for image in images:
    image_url = '/'.join(image['src'].split('/')[0:-1])
    file_name = image_url.split('/')[-1]
    print("Starting download", file_name)
    r = requests.get(image_url, stream=True)
    with open(file_name, 'wb') as f:
        for chunk in r.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
    print("Downloaded", file_name)

print('Done!')
